{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push data from **Cognite Data Fusion** to a **Power BI** `push` or `streaming` dataset\n",
    "On this notebook, we will be pushing data from **Cognite Data Fusion** to a **Power BI** `push` or `streaming` dataset. On this example a new `push` dataset was created in **Power BI** with the following schema:\n",
    "```json\n",
    "[\n",
    "    {\n",
    "        \"timestamp\" :\"2023-11-07T18:46:22.462Z\",\n",
    "        \"value\" :98.6,\n",
    "        \"unit\" :\"AAAAA555555\",\n",
    "        \"sensor\" :\"AAAAA555555\",\n",
    "        \"tag\" :\"AAAAA555555\"\n",
    "    }\n",
    "]\n",
    "```\n",
    "\n",
    "To create a new `push` or `streaming` dataset in **Power BI** follow the steps below:\n",
    "1. Go to [Power BI](https://app.powerbi.com/) and login with your credentials\n",
    "2. Go to your workspace and click on `+ New` and then `Streaming dataset`\n",
    "3. Select `API` and click on `Next`\n",
    "4. Give your dataset a name and configure the desired schema\n",
    "\n",
    "Copy the URL of the dataset as we will be using it later on this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "To run this notebook you will need to install the following dependencies:\n",
    "- [Cognite Python SDK](https://pypi.org/project/cognite-sdk/)\n",
    "- [dotenv](https://pypi.org/project/python-dotenv/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from cognite.client import CogniteClient, ClientConfig\n",
    "from cognite.client.credentials import OAuthClientCredentials\n",
    "from dotenv import load_dotenv\n",
    "from cognite.client.data_classes import DataPointSubscriptionCreate\n",
    "from cognite.client.data_classes import filters as f\n",
    "from cognite.client.data_classes import ClientCredentials\n",
    "from cognite.client.data_classes.datapoints_subscriptions import DatapointSubscriptionFilterProperties as p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credentials\n",
    "To authenticate to Cognite Data Fusion you will need credentials. This notebook expects the credentials to be stored in an `.env` file in the same directory as the notebook. The `.env` file should look like this:\n",
    "```bash\n",
    "CLIENT_ID=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "TENANT_ID=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "CDF_CLUSTER=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "CDF_PROJECT=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "CLIENT_SECRET=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\n",
    "```\n",
    "Replace the `x` with your own credentials. In this case, I'm using client credentials flow, but you can use other flows as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "client_id = os.environ[\"CLIENT_ID\"]\n",
    "client_secret = os.environ[\"CLIENT_SECRET\"]\n",
    "tenant_id = os.environ[\"TENANT_ID\"]\n",
    "cluster = os.environ[\"CDF_CLUSTER\"]\n",
    "project = os.environ[\"CDF_PROJECT\"]\n",
    "\n",
    "scopes = [f\"https://{cluster}.cognitedata.com/.default\"]\n",
    "base_url = f\"https://{cluster}.cognitedata.com\"\n",
    "authority_uri = f\"https://login.microsoftonline.com/{tenant_id}\"\n",
    "\n",
    "creds = OAuthClientCredentials(\n",
    "    token_url=f\"{authority_uri}/oauth2/v2.0/token\",\n",
    "    scopes=scopes,\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    ")\n",
    "\n",
    "cnf = ClientConfig(\n",
    "    client_name=\"powerbi-tutorial\",\n",
    "    base_url=base_url,\n",
    "    project=project,\n",
    "    credentials=creds,\n",
    ")\n",
    "\n",
    "client = CogniteClient(config=cnf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Subscription\n",
    "We'll start by creating a subscription to time series. This is a new feature of Cognite Data Fusion that allows you to subscribe to changes in the time series. You can read more about it [here](https://api-docs.cognite.com/20230101-beta/tag/Data-point-subscriptions).\n",
    "\n",
    "In this example, I'm subscribing to all time series that have a prefix of \"EVE\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_filter = f.Prefix(\n",
    "    property=p.external_id,\n",
    "    value=\"EVE\"\n",
    ")\n",
    "sub = DataPointSubscriptionCreate(\n",
    "    external_id=\"powerbi-subscription-test\",\n",
    "    name=\"PowerBI Subscription Test\",\n",
    "    description=\"Subscription for PowerBI\",\n",
    "    partition_count=1,\n",
    "    filter=my_filter\n",
    ")\n",
    "client.time_series.subscriptions.create(sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Function `handle`\n",
    "Here we'll create a function called `handle` where the logic that fetches updates from CDF and pushes it to Power BI will be implemented. To manage the state of the subscription cursor we'll be using a string time series.\n",
    "\n",
    "The logic of the function is as follows:\n",
    "1. Fetch the latest cursor from the time series (if the time series doesn't exist, create it)\n",
    "2. Fetch the time series included in the subscription\n",
    "3. Fetch the metadata of the time series\n",
    "4. Fetch the updates from the subscription\n",
    "5. Check if there are any updates (if there are no updates, we can skip the rest of the logic)\n",
    "6. Create the payload to be pushed to Power BI (conforming to the schema of the dataset)\n",
    "7. Push the payload to Power BI (if the push succeeds, insert the new cursor in the time series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle(client, data, secrets):\n",
    "    import requests\n",
    "    import json\n",
    "    from datetime import datetime, timezone\n",
    "    from cognite.client.data_classes import TimeSeries\n",
    "    from cognite.client.utils import ms_to_datetime, datetime_to_ms\n",
    "\n",
    "    SUBSCRIPTION_EXTERNAL_ID = \"powerbi-subscription-test\"\n",
    "    STATE_EXTERNAL_ID = \"powerbi-subscription-test-state\"\n",
    "\n",
    "    # 1) Get latest cursor from state store\n",
    "    state_ts = client.time_series.retrieve(external_id=STATE_EXTERNAL_ID)\n",
    "    if state_ts is None:\n",
    "        # TimeSeries does not exist, so we create it\n",
    "        client.time_series.create(\n",
    "            TimeSeries(\n",
    "                name=\"PowerBI Subscription Test State\",\n",
    "                description=\"State store for PowerBI subscription\",\n",
    "                external_id=STATE_EXTERNAL_ID,\n",
    "                is_string=True,\n",
    "            )\n",
    "        )\n",
    "        print(\"Time Series created to store state\")\n",
    "        # cursor is None, so we fetch all data points\n",
    "        cursor = None\n",
    "    else:\n",
    "        # Fetch the latest cursor from the state store\n",
    "        cursor_dp = client.time_series.data.retrieve_latest(\n",
    "            external_id=STATE_EXTERNAL_ID\n",
    "        )\n",
    "        if len(cursor_dp.value) == 0:\n",
    "            # cursor is None, so we fetch all data points\n",
    "            print(\"No cursor found in state store\")\n",
    "            cursor = None\n",
    "        else:\n",
    "            cursor = cursor_dp.value[0]\n",
    "\n",
    "    # 2) Fetch the time series included in the subscription\n",
    "    ts_items = client.time_series.subscriptions.list_member_time_series(\n",
    "        external_id=SUBSCRIPTION_EXTERNAL_ID, limit=None\n",
    "    )\n",
    "\n",
    "    # 3) Fetch the time series metadata\n",
    "    ts_list = client.time_series.retrieve_multiple(ids=[ts.id for ts in ts_items])\n",
    "\n",
    "    # 4) Fetch the updates from the subscription\n",
    "    def fetch_data_points_updates(external_id, cursor=None):\n",
    "        update_data = []\n",
    "        for batch in client.time_series.subscriptions.iterate_data(\n",
    "            external_id=external_id, cursor=cursor\n",
    "        ):\n",
    "            update_data = update_data + batch.updates\n",
    "            cursor = batch.cursor\n",
    "            if not batch.has_next:\n",
    "                break\n",
    "        return update_data, cursor\n",
    "\n",
    "    update_data, cursor = fetch_data_points_updates(\n",
    "        external_id=SUBSCRIPTION_EXTERNAL_ID, cursor=cursor\n",
    "    )\n",
    "\n",
    "    # 5) We can stop here if there are no updates\n",
    "    # We don't need to update the cursor in the state store\n",
    "    if len(update_data) == 0:\n",
    "        print(\"No updates found\")\n",
    "        return None\n",
    "\n",
    "    # 6) Create the payload for PowerBI\n",
    "    def create_powerbi_payload(update_data, ts_list):\n",
    "        # Convert ms since epoch to ISO 8601 format (used by Power BI)\n",
    "        def convert_ms_to_iso(ms_since_epoch):\n",
    "            dt_object = ms_to_datetime(ms_since_epoch)\n",
    "            return dt_object.isoformat(timespec=\"milliseconds\").replace(\"+00:00\", \"Z\")\n",
    "\n",
    "        data = []\n",
    "        for item in update_data:\n",
    "            # we are only interested in upserts\n",
    "            upsert = item.upserts\n",
    "            # find the time series metadata\n",
    "            ts = next((ts for ts in ts_list if ts.id == upsert.id), None)\n",
    "            data.append(\n",
    "                {\n",
    "                    \"timestamp\": convert_ms_to_iso(upsert.timestamp[0]),\n",
    "                    \"value\": upsert.value[0],\n",
    "                    \"unit\": ts.unit,\n",
    "                    \"sensor\": ts.name,\n",
    "                    \"tag\": ts.external_id,\n",
    "                }\n",
    "            )\n",
    "        return data\n",
    "\n",
    "    data = create_powerbi_payload(update_data, ts_list)\n",
    "\n",
    "    # 7) Push the data to PowerBI URL from secrets\n",
    "    url = secrets.get(\"url\")\n",
    "\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    response = requests.request(\n",
    "        method=\"POST\", url=url, headers=headers, data=json.dumps(data)\n",
    "    )\n",
    "    if response.status_code == 200:\n",
    "        print(\"Data pushed successfully to Power BI\")\n",
    "        # Update the cursor in the state store\n",
    "        now = datetime_to_ms(datetime.now(timezone.utc))\n",
    "        client.time_series.data.insert(\n",
    "            external_id=STATE_EXTERNAL_ID,\n",
    "            datapoints=[{\"timestamp\": now, \"value\": cursor}],\n",
    "        )\n",
    "    else:\n",
    "        print(\"Error pushing data to Power BI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Function `handle`\n",
    "Before deploying the function, we can test it locally. Replace the `url` variable with the URL of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle(client, {}, {\"url\": \"https://api.powerbi.com/beta/...\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Cognite Function\n",
    "Now that we have tested the function locally, we can deploy it to Cognite Functions. Replace the `url` variable with the URL of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function = client.functions.create(\n",
    "    external_id=\"push-to-powerbi\",\n",
    "    name=\"Push to Power BI\",\n",
    "    function_handle=handle,\n",
    "    secrets={\"url\": \"https://api.powerbi.com/beta/...\"},\n",
    ")\n",
    "function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Function Schedule\n",
    "We need to wait a little bit until the function is successfully deployed. Once it is deployed, we can create a schedule for it. This schedule will run the function every 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = client.functions.schedules.create(\n",
    "    name=\"Push data to Power BI\",\n",
    "    function_external_id=\"push-to-powerbi\",\n",
    "    cron_expression=\"*/5 * * * *\",\n",
    "    client_credentials=ClientCredentials(\n",
    "        client_id=client_id,\n",
    "        client_secret=client_secret\n",
    "    ),\n",
    "    description=\"Push data to Power BI every 5 minutes\",\n",
    "    data={},\n",
    ")\n",
    "schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
